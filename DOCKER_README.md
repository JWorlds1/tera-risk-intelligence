# üê≥ Docker Setup f√ºr Climate Conflict Pipeline

## √úbersicht

Dieses Docker-Setup erm√∂glicht es, die gesamte Pipeline in einem Container auszuf√ºhren und mit Studenten zu teilen oder auf GitHub/GitLab zu pushen.

## üöÄ Schnellstart

### 1. Repository klonen/erhalten

```bash
git clone <repository-url>
cd Geospatial_Intelligence
```

### 2. Docker Container bauen und starten

```bash
# Pipeline einmalig ausf√ºhren
docker-compose -f docker-compose.pipeline.yml up --build pipeline

# Oder im Hintergrund
docker-compose -f docker-compose.pipeline.yml up -d pipeline
```

### 3. Dashboard anzeigen

```bash
# Dashboard starten
docker-compose -f docker-compose.pipeline.yml up dashboard

# Dashboard √∂ffnen im Browser
# http://localhost:5000
```

### 4. Extraktionen testen

```bash
# Test-Script ausf√ºhren
docker-compose -f docker-compose.pipeline.yml run --rm pipeline python test_extraction.py
```

## üì¶ Docker Services

### Pipeline Service
F√ºhrt einmalig einen vollst√§ndigen Crawl durch:
```bash
docker-compose -f docker-compose.pipeline.yml up pipeline
```

### Scheduler Service
F√ºhrt automatisiertes Crawling durch (t√§glich, alle 6 Stunden):
```bash
docker-compose -f docker-compose.pipeline.yml up scheduler
```

### Dashboard Service
Web-Dashboard zum Anzeigen der Daten:
```bash
docker-compose -f docker-compose.pipeline.yml up dashboard
```

## üîß Manuelle Docker-Befehle

### Container bauen
```bash
docker build -t climate-pipeline .
```

### Container ausf√ºhren
```bash
# Pipeline ausf√ºhren
docker run --rm -v $(pwd)/data:/app/data climate-pipeline python run_pipeline.py

# Test ausf√ºhren
docker run --rm -v $(pwd)/data:/app/data climate-pipeline python test_extraction.py

# Dashboard starten
docker run --rm -p 5000:5000 -v $(pwd)/data:/app/data climate-pipeline python dashboard_viewer.py
```

### In Container einloggen
```bash
docker-compose -f docker-compose.pipeline.yml exec pipeline bash
```

## üìä Daten anzeigen

### 1. Web-Dashboard
```bash
# Dashboard starten
docker-compose -f docker-compose.pipeline.yml up dashboard

# Im Browser √∂ffnen
open http://localhost:5000
```

### 2. Test-Script
```bash
docker-compose -f docker-compose.pipeline.yml run --rm pipeline python test_extraction.py
```

### 3. Datenbank direkt abfragen
```bash
# SQLite Shell im Container
docker-compose -f docker-compose.pipeline.yml exec pipeline sqlite3 /app/data/climate_conflict.db

# Oder lokal (wenn SQLite installiert)
sqlite3 ./data/climate_conflict.db
```

### 4. Datenbank-Statistiken
```sql
-- Gesamt Records
SELECT COUNT(*) FROM records;

-- Records pro Quelle
SELECT source_name, COUNT(*) FROM records GROUP BY source_name;

-- Neueste Records
SELECT source_name, title, publish_date, region 
FROM records 
ORDER BY fetched_at DESC 
LIMIT 10;
```

## üìÅ Daten-Persistenz

Alle Daten werden im `./data` Verzeichnis gespeichert:
- `climate_conflict.db` - SQLite Datenbank
- `json/` - JSON-Dateien
- `csv/` - CSV-Dateien
- `parquet/` - Parquet-Dateien

Dieses Verzeichnis wird als Volume gemountet, sodass Daten auch nach Container-Stopp erhalten bleiben.

## üß™ Testing

### Vollst√§ndiger Test-Workflow

```bash
# 1. Container bauen
docker-compose -f docker-compose.pipeline.yml build

# 2. Pipeline ausf√ºhren (Extraktion)
docker-compose -f docker-compose.pipeline.yml up pipeline

# 3. Extraktionen testen
docker-compose -f docker-compose.pipeline.yml run --rm pipeline python test_extraction.py

# 4. Dashboard starten
docker-compose -f docker-compose.pipeline.yml up dashboard
```

### Automatischer Test (f√ºr CI/CD)

```bash
# Setze AUTO_RUN Flag
docker run --rm \
  -e AUTO_RUN=true \
  -v $(pwd)/data:/app/data \
  climate-pipeline python test_extraction.py
```

## üîç Debugging

### Logs anzeigen
```bash
# Alle Logs
docker-compose -f docker-compose.pipeline.yml logs

# Pipeline Logs
docker-compose -f docker-compose.pipeline.yml logs pipeline

# Dashboard Logs
docker-compose -f docker-compose.pipeline.yml logs dashboard
```

### Container-Logs folgen
```bash
docker-compose -f docker-compose.pipeline.yml logs -f pipeline
```

### In Container einloggen
```bash
docker-compose -f docker-compose.pipeline.yml exec pipeline bash

# Dann im Container:
python test_extraction.py
python run_pipeline.py
sqlite3 /app/data/climate_conflict.db
```

## üì§ F√ºr GitHub/GitLab vorbereiten

### 1. .gitignore pr√ºfen
Stelle sicher, dass folgende Dateien ignoriert werden:
```
data/*.db
data/*.json
data/*.csv
data/*.parquet
__pycache__/
*.pyc
.env
```

### 2. README.md aktualisieren
F√ºge Docker-Anleitung hinzu (siehe DOCKER_README.md)

### 3. Repository pushen
```bash
git add .
git commit -m "Add Docker setup for pipeline"
git push origin main
```

### 4. Studenten k√∂nnen dann:
```bash
git clone <repository-url>
cd Geospatial_Intelligence
docker-compose -f docker-compose.pipeline.yml up --build pipeline
```

## üéì F√ºr Studenten - Einfache Anleitung

### Schritt 1: Repository klonen
```bash
git clone <repository-url>
cd Geospatial_Intelligence
```

### Schritt 2: Docker installieren
- **macOS**: [Docker Desktop](https://www.docker.com/products/docker-desktop)
- **Windows**: [Docker Desktop](https://www.docker.com/products/docker-desktop)
- **Linux**: `sudo apt-get install docker.io docker-compose`

### Schritt 3: Pipeline ausf√ºhren
```bash
docker-compose -f docker-compose.pipeline.yml up --build pipeline
```

### Schritt 4: Daten anzeigen
```bash
# Option 1: Web-Dashboard
docker-compose -f docker-compose.pipeline.yml up dashboard
# Dann Browser √∂ffnen: http://localhost:5000

# Option 2: Test-Script
docker-compose -f docker-compose.pipeline.yml run --rm pipeline python test_extraction.py
```

## üêõ Troubleshooting

### Container startet nicht
```bash
# Logs pr√ºfen
docker-compose -f docker-compose.pipeline.yml logs

# Container neu bauen
docker-compose -f docker-compose.pipeline.yml build --no-cache
```

### Datenbank nicht gefunden
```bash
# Pr√ºfe ob data-Verzeichnis existiert
ls -la ./data

# Erstelle falls nicht vorhanden
mkdir -p ./data
```

### Port bereits belegt
```bash
# √Ñndere Port in docker-compose.pipeline.yml
ports:
  - "5001:5000"  # Statt 5000:5000
```

### Permission-Denied Fehler
```bash
# Pr√ºfe Schreibrechte
chmod -R 755 ./data

# Oder √§ndere Owner
sudo chown -R $USER:$USER ./data
```

## üìù Beispiel-Workflow

```bash
# 1. Projekt klonen
git clone <repo>
cd Geospatial_Intelligence

# 2. Pipeline ausf√ºhren
docker-compose -f docker-compose.pipeline.yml up pipeline

# 3. Warten bis fertig (oder im Hintergrund: -d)

# 4. Daten testen
docker-compose -f docker-compose.pipeline.yml run --rm pipeline python test_extraction.py

# 5. Dashboard starten
docker-compose -f docker-compose.pipeline.yml up dashboard

# 6. Browser √∂ffnen: http://localhost:5000
```

## ‚úÖ Checkliste f√ºr Deployment

- [ ] Docker installiert
- [ ] Repository geklont
- [ ] `docker-compose.pipeline.yml` vorhanden
- [ ] `Dockerfile` vorhanden
- [ ] `.dockerignore` vorhanden
- [ ] `data/` Verzeichnis erstellt
- [ ] Pipeline erfolgreich ausgef√ºhrt
- [ ] Dashboard l√§uft
- [ ] Daten sichtbar

## üéâ Fertig!

Das System ist jetzt bereit f√ºr:
- ‚úÖ Teilen mit Studenten
- ‚úÖ Deployment auf GitHub/GitLab
- ‚úÖ Automatisiertes Testing
- ‚úÖ Einfache Reproduzierbarkeit

