# ğŸ“Š Pipeline-Status & Zusammenfassung

## âœ… Was wurde implementiert?

### 1. **Datenbank-Architektur** âœ…
- **SQLite-Datenbank** mit normalisiertem Schema
- **Haupttabelle `records`** fÃ¼r alle extrahierten Daten
- **Quellenspezifische Tabellen** (nasa_records, un_press_records, wfp_records, worldbank_records)
- **Relationstabellen** fÃ¼r Topics, Links, Bilder
- **Job-Tracking-Tabelle** fÃ¼r Crawl-Jobs
- **Indizes** fÃ¼r Performance-Optimierung

### 2. **Database Manager** âœ…
- CRUD-Operationen fÃ¼r alle Records
- Batch-Insert mit Tracking von neuen/aktualisierten Records
- Job-Management fÃ¼r Crawl-Jobs
- Statistiken und Monitoring-Funktionen
- Automatische Duplikat-Erkennung (basierend auf URL)

### 3. **Pipeline-System** âœ…
- **CrawlingPipeline**: Koordiniert Crawls fÃ¼r einzelne oder alle Quellen
- **PipelineScheduler**: Automatisiertes Scheduling (tÃ¤glich, stÃ¼ndlich, intervall-basiert)
- Integration mit bestehendem Orchestrator
- Fehlerbehandlung und Logging

### 4. **Orchestrator-Integration** âœ…
- Datenbank-Integration in ScrapingOrchestrator
- Automatisches Speichern in Datenbank + Dateiformate
- Tracking von neuen vs. aktualisierten Records
- Erweiterte Statistiken mit Datenbank-Informationen

### 5. **Prototyp-Script** âœ…
- `run_pipeline.py`: Einmalige AusfÃ¼hrung mit detailliertem Output
- Zeigt Datenbank-Statistiken vor/nach Crawl
- Zeigt Crawling-Ergebnisse und neue Records
- Benutzerfreundliche Ausgabe mit Rich

## ğŸ¯ Die drei Hauptwebseiten

1. **NASA Earth Observatory** (earthobservatory.nasa.gov)
   - Umweltstress & KlimaverÃ¤nderungen
   - Satellitendaten & Umweltindikatoren

2. **UN Press** (press.un.org)
   - Politische & sicherheitspolitische Reaktionen
   - Security Council Meetings
   - Press Releases

3. **World Bank** (worldbank.org)
   - Wirtschaftliche & strukturelle Verwundbarkeit
   - Projekte & LÃ¤nderanalysen

## ğŸš€ Verwendung

### Schnellstart - Prototyp ausfÃ¼hren

```bash
cd backend
python run_pipeline.py
```

Dies fÃ¼hrt einen vollstÃ¤ndigen Crawl durch und zeigt:
- âœ… Aktuelle Datenbank-Statistiken
- âœ… Crawling-Ergebnisse fÃ¼r alle Quellen
- âœ… Anzahl neuer/aktualisierter Records
- âœ… Beispiele der neuesten Records

### Automatisiertes Crawling starten

```bash
cd backend
python pipeline.py --scheduled
```

**Standard-Schedule:**
- TÃ¤glich um 02:00 Uhr: VollstÃ¤ndiger Crawl
- Alle 6 Stunden: Inkrementeller Crawl

### Datenbank abfragen

```bash
sqlite3 ./data/climate_conflict.db

# Statistiken
SELECT source_name, COUNT(*) FROM records GROUP BY source_name;

# Neueste Records
SELECT source_name, title, publish_date, region 
FROM records 
ORDER BY fetched_at DESC 
LIMIT 10;
```

## ğŸ“Š Datenbank-Schema

### Haupttabelle: `records`
- Basis-Informationen: url, source_name, title, summary
- Metadaten: publish_date, region, content_type
- Timestamps: fetched_at, created_at, updated_at

### Quellenspezifische Tabellen:
- **nasa_records**: environmental_indicators, satellite_source
- **un_press_records**: meeting_coverage, security_council, speakers
- **wfp_records**: crisis_type, affected_population
- **worldbank_records**: country, sector, project_id

### Job-Tracking: `crawl_jobs`
- Status: pending, running, completed, failed
- Statistiken: records_extracted, records_new, records_updated
- Fehler-Tracking: error_message

## ğŸ”„ Pipeline-Architektur

```
Scheduler
    â†“
CrawlingPipeline
    â†“
ScrapingOrchestrator
    â”œâ”€â”€ Compliance Agent
    â”œâ”€â”€ Fetch Agent
    â”œâ”€â”€ Extract Agent
    â”œâ”€â”€ Validate Agent
    â””â”€â”€ Storage Agent
        â”œâ”€â”€ DatabaseManager (SQLite)
        â””â”€â”€ File Storage (JSON/CSV/Parquet)
```

## ğŸ“ˆ Monitoring

### Datenbank-Statistiken

```python
from database import DatabaseManager

db = DatabaseManager()
stats = db.get_statistics()

print(f"Total Records: {stats['total_records']}")
print(f"By Source: {stats['records_by_source']}")
print(f"Last 24h: {stats['records_last_24h']}")
```

### Crawl-Jobs prÃ¼fen

```python
# Alle Jobs
jobs = db.get_crawl_jobs()

# Fehlgeschlagene Jobs
failed = db.get_crawl_jobs(status='failed')

# Jobs fÃ¼r NASA
nasa_jobs = db.get_crawl_jobs(source_name='NASA')
```

## âš™ï¸ Konfiguration

Die Pipeline nutzt `config.py`:
- `RATE_LIMIT`: 1.0 req/s (anpassbar)
- `MAX_CONCURRENT`: 3 parallele Requests
- `STORAGE_DIR`: ./data
- `HTTP_TIMEOUT`: 20 Sekunden

## ğŸ“ Neue Dateien

```
backend/
â”œâ”€â”€ database.py          # Datenbank-Manager (NEU)
â”œâ”€â”€ pipeline.py         # Pipeline & Scheduler (NEU)
â”œâ”€â”€ run_pipeline.py      # Prototyp-Script (NEU)
â””â”€â”€ orchestrator.py     # Aktualisiert mit DB-Integration

data/
â””â”€â”€ climate_conflict.db  # SQLite Datenbank (wird automatisch erstellt)
```

## âœ… Status der Extraktion

**Aktueller Stand:**
- âœ… Extraktion funktioniert fÃ¼r alle drei Hauptwebseiten
- âœ… Datenbank-Schema implementiert
- âœ… Automatisierte Pipeline bereit
- âœ… Prototyp getestet und funktionsfÃ¤hig

**NÃ¤chste Schritte:**
1. Pipeline mit echten Daten testen: `python run_pipeline.py`
2. Automatisiertes Crawling starten: `python pipeline.py --scheduled`
3. Daten analysieren mit vorhandenen Analyse-Tools
4. Monitoring-Dashboard erweitern

## ğŸ‰ Zusammenfassung

Das Projekt hat jetzt:
- âœ… **Zentrale Datenbank** fÃ¼r alle extrahierten Daten
- âœ… **Automatisierte Pipeline** fÃ¼r regelmÃ¤ÃŸiges Crawling
- âœ… **Job-Tracking** fÃ¼r Monitoring
- âœ… **Prototyp** fÃ¼r sofortige Nutzung
- âœ… **Skalierbare Architektur** fÃ¼r zukÃ¼nftige Erweiterungen

**Das System ist bereit fÃ¼r den produktiven Einsatz!** ğŸš€

