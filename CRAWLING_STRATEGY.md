# üï∑Ô∏è Crawling-Strategie - Verbesserungen

## üîç Aktuelle Probleme

1. **√úbersichtsseiten statt Artikel**: Die aktuellen URLs f√ºhren zu √úbersichtsseiten, nicht zu einzelnen Artikeln
2. **Fehlende Artikel-Discovery**: Wir crawlen nicht die Links auf den √úbersichtsseiten
3. **URL-Patterns √§ndern sich**: Webseiten-Strukturen √§ndern sich h√§ufig

## ‚úÖ L√∂sungsans√§tze

### Strategie 1: Artikel-Discovery (Implementiert)

**Wie es funktioniert:**
1. Crawle √úbersichtsseiten
2. Finde alle Links die zu Artikeln f√ºhren
3. Crawle dann die einzelnen Artikel

**Vorteile:**
- Findet automatisch neue Artikel
- Funktioniert auch wenn URLs sich √§ndern
- Skalierbar

### Strategie 2: RSS Feeds nutzen

**Viele Webseiten haben RSS Feeds:**
- NASA: `https://earthobservatory.nasa.gov/feeds/earth.rss`
- UN Press: `https://press.un.org/en/rss.xml`
- World Bank: `https://www.worldbank.org/en/news/rss`

**Vorteile:**
- Strukturierte Daten
- Immer aktuelle Artikel
- Einfacher zu parsen

### Strategie 3: Sitemaps nutzen

**Viele Webseiten haben Sitemaps:**
- `https://earthobservatory.nasa.gov/sitemap.xml`
- `https://press.un.org/sitemap.xml`
- `https://www.worldbank.org/sitemap.xml`

**Vorteile:**
- Alle URLs auf einmal
- Strukturiert
- Offiziell unterst√ºtzt

## üöÄ Implementierung

### Schritt 1: Smart Crawler (‚úÖ Erstellt)

```bash
python smart_crawler.py
```

- Findet Artikel-URLs automatisch
- Crawlt dann die Artikel
- Speichert in Datenbank

### Schritt 2: RSS Feeds (‚è≥ N√§chster Schritt)

```python
# RSS Parser implementieren
- Parse RSS Feeds
- Extrahiere Artikel-URLs
- Crawle Artikel
```

### Schritt 3: Sitemaps (‚è≥ Optional)

```python
# Sitemap Parser
- Parse Sitemap XML
- Filter nach relevanten URLs
- Crawle Artikel
```

## üìä Was wird aktuell gecrawlt?

### Aus den bestehenden Daten:

**NASA (2 Records):**
- √úbersichtsseiten (Features, World of Change)
- Keine einzelnen Artikel

**UN Press (2 Records):**
- √úbersichtsseiten
- Ein Security Council Meeting

**World Bank (1 Record):**
- √úbersichtsseite

### Mit Smart Crawler:

**Erwartete Ergebnisse:**
- 20-50 Artikel pro Quelle
- Echte Inhalte mit Details
- Bessere Extraktion

## üéØ N√§chste Schritte

1. ‚úÖ **Smart Crawler** erstellt
2. ‚è≥ **RSS Feeds** implementieren
3. ‚è≥ **Sitemaps** nutzen
4. ‚è≥ **Bessere Extraktion** f√ºr Artikel-Inhalte
5. ‚è≥ **Volltext-Extraktion** implementieren

## üîß Verbesserungen f√ºr Extraktion

### NASA:
- Extrahiere Bild-URLs mit Koordinaten
- Nutze Metadaten aus Bildern
- Extrahiere Datum-Ranges f√ºr "World of Change"

### UN Press:
- Extrahiere vollst√§ndigen Text von Press Releases
- Identifiziere L√§nder aus Text
- Extrahiere Zitate und Statements

### World Bank:
- Extrahiere Projekt-Details
- Nutze strukturierte Daten (JSON-LD)
- Extrahiere Finanzierungs-Betr√§ge



