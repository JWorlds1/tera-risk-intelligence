# ğŸš€ Automatisierte Crawling-Pipeline

## Ãœbersicht

Die Pipeline ermÃ¶glicht automatisiertes Crawling der drei Hauptwebseiten:
- **NASA Earth Observatory** (earthobservatory.nasa.gov)
- **UN Press** (press.un.org)
- **World Bank** (worldbank.org)

Alle extrahierten Daten werden in einer SQLite-Datenbank gespeichert und kÃ¶nnen fÃ¼r Analysen verwendet werden.

## ğŸ—ï¸ Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Pipeline Scheduler                          â”‚
â”‚  (TÃ¤glich 02:00, alle 6 Stunden, etc.)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CrawlingPipeline                               â”‚
â”‚  - Erstellt Crawl-Jobs                                   â”‚
â”‚  - Koordiniert Orchestrator                             â”‚
â”‚  - Speichert in Datenbank                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ScrapingOrchestrator                             â”‚
â”‚  - Compliance Agent (robots.txt, Rate Limiting)         â”‚
â”‚  - Fetch Agent (HTTP Requests)                           â”‚
â”‚  - Extract Agent (Content Parsing)                      â”‚
â”‚  - Validate Agent (Data Validation)                    â”‚
â”‚  - Storage Agent (Files + Database)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DatabaseManager                                â”‚
â”‚  - SQLite Datenbank                                      â”‚
â”‚  - Normalisierte Tabellen                                â”‚
â”‚  - Job-Tracking                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Datenbank-Schema

### Haupttabellen

1. **records** - Alle extrahierten Records
   - `id`, `url`, `source_domain`, `source_name`
   - `title`, `summary`, `publish_date`, `region`
   - `content_type`, `language`, `full_text`
   - `fetched_at`, `created_at`, `updated_at`

2. **record_topics** - Topics/Tags pro Record
3. **record_links** - Links pro Record
4. **record_images** - Bilder pro Record

### Quellenspezifische Tabellen

- **nasa_records** - NASA-spezifische Daten (environmental_indicators, satellite_source)
- **un_press_records** - UN-spezifische Daten (meeting_coverage, security_council, speakers)
- **wfp_records** - WFP-spezifische Daten (crisis_type, affected_population)
- **worldbank_records** - World Bank-spezifische Daten (country, sector, project_id)

### Job-Tracking

- **crawl_jobs** - Tracking aller Crawl-Jobs mit Status, Statistiken, Fehlern

## ğŸš€ Verwendung

### 1. Prototyp ausfÃ¼hren (einmalig)

```bash
cd backend
python run_pipeline.py
```

Dies fÃ¼hrt einen vollstÃ¤ndigen Crawl fÃ¼r alle drei Quellen durch und zeigt:
- Aktuelle Datenbank-Statistiken
- Crawling-Ergebnisse
- Neue Records
- Aktualisierte Records

### 2. Automatisiertes Crawling starten

```bash
cd backend
python pipeline.py --scheduled
```

Dies startet den Scheduler mit folgenden Einstellungen:
- **TÃ¤glich um 02:00 Uhr**: VollstÃ¤ndiger Crawl aller Quellen
- **Alle 6 Stunden**: Inkrementeller Crawl

### 3. Manueller Crawl fÃ¼r spezifische Quellen

```python
from config import Config
from pipeline import CrawlingPipeline
import asyncio

config = Config()
pipeline = CrawlingPipeline(config)

# Nur NASA und UN Press crawlen
results = asyncio.run(pipeline.run_full_crawl(['NASA', 'UN Press']))
```

### 4. Datenbank abfragen

```bash
# SQLite Shell Ã¶ffnen
sqlite3 ./data/climate_conflict.db

# Alle Records anzeigen
SELECT source_name, COUNT(*) FROM records GROUP BY source_name;

# Neueste Records
SELECT source_name, title, publish_date, region 
FROM records 
ORDER BY fetched_at DESC 
LIMIT 10;

# Records mit Topics
SELECT r.title, GROUP_CONCAT(rt.topic) as topics
FROM records r
LEFT JOIN record_topics rt ON r.id = rt.record_id
GROUP BY r.id
LIMIT 10;
```

## ğŸ“ˆ Monitoring

### Datenbank-Statistiken abrufen

```python
from database import DatabaseManager

db = DatabaseManager()
stats = db.get_statistics()

print(f"Total Records: {stats['total_records']}")
print(f"Records by Source: {stats['records_by_source']}")
print(f"Records last 24h: {stats['records_last_24h']}")
```

### Crawl-Jobs prÃ¼fen

```python
from database import DatabaseManager

db = DatabaseManager()

# Alle Jobs
jobs = db.get_crawl_jobs()

# Nur fehlgeschlagene Jobs
failed_jobs = db.get_crawl_jobs(status='failed')

# Jobs fÃ¼r eine bestimmte Quelle
nasa_jobs = db.get_crawl_jobs(source_name='NASA')
```

## âš™ï¸ Konfiguration

Die Pipeline verwendet die Konfiguration aus `config.py`:

- **RATE_LIMIT**: Rate Limiting (Standard: 1.0 req/s)
- **MAX_CONCURRENT**: Maximale parallele Requests (Standard: 3)
- **STORAGE_DIR**: Verzeichnis fÃ¼r Dateien (Standard: ./data)
- **HTTP_TIMEOUT**: Timeout fÃ¼r HTTP Requests (Standard: 20s)

## ğŸ”„ Scheduler-Anpassung

Die Scheduler-Einstellungen kÃ¶nnen in `pipeline.py` angepasst werden:

```python
# TÃ¤glich um 03:00 Uhr
scheduler.schedule_daily_crawl("03:00")

# StÃ¼ndlich um Minute 15
scheduler.schedule_hourly_crawl(15)

# Alle 4 Stunden
scheduler.schedule_interval_crawl(4)
```

## ğŸ“ Dateistruktur

```
backend/
â”œâ”€â”€ database.py          # Datenbank-Manager
â”œâ”€â”€ pipeline.py         # Pipeline & Scheduler
â”œâ”€â”€ orchestrator.py     # Orchestrator (aktualisiert)
â”œâ”€â”€ run_pipeline.py      # Prototyp-Script
â””â”€â”€ data/
    â””â”€â”€ climate_conflict.db  # SQLite Datenbank
```

## âœ… Status

- âœ… Datenbank-Schema implementiert
- âœ… Database Manager mit CRUD-Operationen
- âœ… Pipeline-Scheduler fÃ¼r automatisiertes Crawling
- âœ… Orchestrator mit Datenbank-Integration
- âœ… Prototyp-Script fÃ¼r einmalige AusfÃ¼hrung
- âœ… Job-Tracking fÃ¼r Monitoring

## ğŸ¯ NÃ¤chste Schritte

1. **Testing**: Pipeline mit echten Daten testen
2. **Monitoring**: Dashboard fÃ¼r Pipeline-Status
3. **Alerting**: Benachrichtigungen bei Fehlern
4. **Optimierung**: Performance-Tuning fÃ¼r groÃŸe Datenmengen
5. **Backup**: Automatische Backups der Datenbank

## ğŸ› Troubleshooting

### Datenbank wird nicht erstellt
- PrÃ¼fe Schreibrechte im `data/` Verzeichnis
- PrÃ¼fe ob SQLite installiert ist

### Pipeline stoppt nicht
- `Ctrl+C` zum Stoppen
- PrÃ¼fe laufende Prozesse: `ps aux | grep python`

### Keine Records in Datenbank
- PrÃ¼fe Logs fÃ¼r Fehler
- PrÃ¼fe ob URLs erreichbar sind
- PrÃ¼fe Rate Limiting Einstellungen

