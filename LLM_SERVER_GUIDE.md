# ğŸ¤– LLM-Server Setup Guide (64-128 GB RAM)

## ğŸ¯ Optimiert fÃ¼r groÃŸe LLM-Inference

Dieser Server ist speziell konfiguriert fÃ¼r:
- âœ… **Lokale LLM-Inference** mit Ollama
- âœ… **GroÃŸe Modelle** (70B+ Parameter)
- âœ… **Dynamisches Internet-Crawling**
- âœ… **Geospatial Intelligence** Analysen

## ğŸ’¾ RAM-Anforderungen

### VerfÃ¼gbare Flavors fÃ¼r LLM:

| Flavor | vCPUs | RAM | Empfohlen fÃ¼r |
|--------|-------|-----|---------------|
| `c1.8xlarge` | 64 | 128 GB | âœ… **Llama2-70B, Mixtral-8x7B** |
| `c1.16xlarge` | 128 | 256 GB | Sehr groÃŸe Modelle |
| `m1.8xlarge` | 32 | 128 GB | Alternative zu c1.8xlarge |
| `c1.3xlarge` | 24 | 48 GB | Llama2-34B, CodeLlama-34B |
| `m1.2xlarge` | 8 | 32 GB | Llama2-7B, Mistral-7B |

### RAM-Verteilung (128 GB Server):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gesamt: 128 GB RAM                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Llama2-70B:        ~40 GB          â”‚
â”‚  PostgreSQL:        ~8 GB           â”‚
â”‚  Crawling:          ~20 GB          â”‚
â”‚  System:            ~10 GB           â”‚
â”‚  Reserve:           ~50 GB           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Server erstellen

```bash
python3 backend/openstack/create_crawling_server.py
```

**Empfohlene Auswahl:**
- **Image:** Ubuntu 22.04 LTS
- **Flavor:** `c1.8xlarge` (64 vCPUs, 128 GB RAM) âœ…
- **Network:** Standard-Network

## ğŸ“¦ Was wird installiert

### LLM-Tools:
- âœ… **Ollama** - Lokale LLM-Inference
- âœ… **Transformers** - Hugging Face Modelle
- âœ… **PyTorch** - Deep Learning Framework
- âœ… **vLLM** - Optimierte Inference (optional)

### Crawling-Tools:
- âœ… Playwright, Selenium
- âœ… httpx, aiohttp
- âœ… BeautifulSoup4, lxml

### Datenbank:
- âœ… PostgreSQL 14+
- âœ… Automatische Backups

## ğŸ”§ Nach Server-Erstellung

### 1. SSH-Verbindung

```bash
ssh ubuntu@<SERVER_IP>
```

### 2. Ollama-Status prÃ¼fen

```bash
systemctl status ollama
curl http://localhost:11434/api/tags  # Liste Modelle
```

### 3. GroÃŸe LLM-Modelle installieren

```bash
# Llama2-70B (~40 GB RAM, ~40 GB Disk)
ollama pull llama2:70b

# Mixtral-8x7B (~26 GB RAM, ~26 GB Disk) - Sehr gut!
ollama pull mixtral:8x7b

# CodeLlama-34B (~20 GB RAM)
ollama pull codellama:34b

# Llama2-34B (~20 GB RAM)
ollama pull llama2:34b

# Liste installierte Modelle
ollama list
```

### 4. LLM testen

```bash
# Interaktiver Test
ollama run llama2:70b "Was ist Geospatial Intelligence?"

# API-Test
curl http://localhost:11434/api/generate -d '{
  "model": "llama2:70b",
  "prompt": "Analysiere Klimarisiken fÃ¼r Ostafrika",
  "stream": false
}'
```

### 5. Python-Integration

```python
import requests

def query_llm(prompt, model="llama2:70b"):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False
        }
    )
    return response.json()["response"]

# Beispiel
result = query_llm("Analysiere Klimarisiken fÃ¼r Somalia")
print(result)
```

## ğŸ¯ FÃ¼r Ihr Geospatial Intelligence Projekt

### Workflow:

```
1. Crawling â†’ PostgreSQL Datenbank
   â””â”€ Gecrawlte Daten (NASA, UN, World Bank)

2. Datenbank â†’ LLM-Context
   â””â”€ Extrahiere relevante Informationen

3. LLM-Inference â†’ Analyse
   â””â”€ Llama2-70B analysiert Daten
   â””â”€ Erstellt Risiko-Vorhersagen

4. Ergebnisse â†’ Datenbank zurÃ¼ck
   â””â”€ Speichere Vorhersagen
   â””â”€ Update Visualisierungen
```

### Beispiel-Integration:

```python
from backend.database import DatabaseManager
import requests

db = DatabaseManager()

# Hole gecrawlte Daten
records = db.get_records_by_region("East Africa")

# Erstelle LLM-Prompt
context = "\n".join([r.summary for r in records[:10]])
prompt = f"""
Analysiere diese Klima- und Konfliktdaten fÃ¼r Ostafrika:
{context}

Erstelle eine Risiko-Vorhersage basierend auf:
1. Klimarisiken
2. Konfliktpotential
3. Wirtschaftliche Faktoren
"""

# LLM-Inference
response = requests.post(
    "http://localhost:11434/api/generate",
    json={
        "model": "llama2:70b",
        "prompt": prompt,
        "stream": False
    }
)

prediction = response.json()["response"]

# Speichere Vorhersage
db.save_prediction("East Africa", prediction)
```

## ğŸ“Š Monitoring

### RAM-Nutzung prÃ¼fen

```bash
# Gesamt-RAM
free -h

# Prozess-spezifisch
ps aux --sort=-%mem | head -10

# Ollama RAM
ps aux | grep ollama | awk '{sum+=$6} END {print sum/1024 " MB"}'
```

### LLM-Performance

```bash
# Ollama Logs
journalctl -u ollama -f

# Test-Geschwindigkeit
time ollama run llama2:70b "Test"
```

## ğŸ” Optimierungen

### RAM-Optimierung

```bash
# Swappiness reduzieren
echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

# Huge Pages aktivieren
echo 'vm.nr_hugepages=1024' | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
```

### Ollama-Konfiguration

```bash
# Konfiguration anpassen
sudo nano /etc/systemd/system/ollama.service.d/override.conf
```

```ini
[Service]
Environment="OLLAMA_NUM_PARALLEL=2"      # Parallele Requests
Environment="OLLAMA_MAX_LOADED_MODELS=1"  # Max gleichzeitige Modelle
Environment="OLLAMA_HOST=0.0.0.0:11434"   # Externer Zugriff
Environment="OLLAMA_KEEP_ALIVE=24h"      # Modelle im RAM behalten
```

```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

## ğŸš¨ Troubleshooting

### Out of Memory

```bash
# PrÃ¼fe verfÃ¼gbaren RAM
free -h

# Verwende quantisierte Modelle
ollama pull llama2:70b-q4_0  # Weniger RAM
```

### Langsame Inference

```bash
# PrÃ¼fe CPU-Auslastung
htop

# Verwende kleinere Modelle fÃ¼r Tests
ollama run mistral:7b  # Schneller
```

## ğŸ“ Zusammenfassung

**FÃ¼r 128 GB RAM Server:**
- âœ… **Llama2-70B** lÃ¤uft perfekt (~40 GB RAM)
- âœ… **Mixtral-8x7B** lÃ¤uft perfekt (~26 GB RAM)
- âœ… Mehrere kleine Modelle parallel mÃ¶glich
- âœ… Optimiert fÃ¼r Geospatial Intelligence

**NÃ¤chste Schritte:**
1. Server erstellen mit `c1.8xlarge` (128 GB RAM)
2. Ollama-Modelle installieren
3. Projekt-Code hochladen
4. Crawling + LLM-Inference starten

Viel Erfolg! ğŸš€

